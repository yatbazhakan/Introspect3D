{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!conda install -c open3d-admin open3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "# import open3d as o3d\n",
    "# print(\"Open3D Version: \" + o3d.__version__)\n",
    "# o3d.visualization.webrtc_server.enable_webrtc()\n",
    "kitti_velodyne_path= r\"D:\\introspectionBase\\datasets\\Kitti\\raw\\training\\velodyne\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        print(avg_out.shape)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        print(max_out.shape)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLocalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels=None,apply=True):\n",
    "        super(NonLocalBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels if inter_channels is not None else in_channels\n",
    "        self.apply = apply\n",
    "\n",
    "        self.g = nn.Conv2d(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.theta = nn.Conv2d(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.phi = nn.Conv2d(in_channels=self.inter_channels, out_channels=self.inter_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.W = nn.Conv2d(in_channels=self.inter_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        nn.init.constant_(self.W.weight, 0)\n",
    "        nn.init.constant_(self.W.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1)\n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "\n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        f_div_C = F.softmax(f, dim=-1)\n",
    "\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "\n",
    "        return z if self.apply else W_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 62, 54])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = torch.ones(1, 256, 1, 1)\n",
    "\n",
    "# Input feature map of shape [1, 256, 62, 54]\n",
    "x = torch.ones(1, 256, 62, 54)\n",
    "\n",
    "# This operation is valid due to broadcasting\n",
    "result = NonLocalBlock(256)(x)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "t = models.resnet18()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotation_matrix_from_corners(corners):\n",
    "    \"\"\"\n",
    "    Get the rotation matrix from 8 corners of a 3D bounding box.\n",
    "    \n",
    "    Parameters:\n",
    "        corners (numpy.ndarray): 3x8 array containing the coordinates of the 8 corners.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: 3x3 rotation matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assume the corners are ordered such that:\n",
    "    # - corners[:, 0] and corners[:, 1] are opposite corners on the \"bottom\" face of the box\n",
    "    # - corners[:, 0] and corners[:, 4] are opposite corners on the \"front\" face of the box\n",
    "    # - corners[:, 0] and corners[:, 3] are opposite corners on the \"left\" face of the box\n",
    "    \n",
    "    # Compute the vectors representing the edges\n",
    "    edge1 = corners[:, 1] - corners[:, 0]  # Vector from corner 0 to corner 1\n",
    "    edge2 = corners[:, 4] - corners[:, 0]  # Vector from corner 0 to corner 4\n",
    "    edge3 = corners[:, 3] - corners[:, 0]  # Vector from corner 0 to corner 3\n",
    "    \n",
    "    # Normalize the vectors\n",
    "    edge1 /= np.linalg.norm(edge1)\n",
    "    edge2 /= np.linalg.norm(edge2)\n",
    "    edge3 /= np.linalg.norm(edge3)\n",
    "    \n",
    "    # Construct the rotation matrix\n",
    "    R = np.column_stack((edge1, edge2, edge3))\n",
    "    \n",
    "    return R\n",
    "def is_inside_ellipse(x, y, a, b):\n",
    "    return (x**2 / a**2) + (y**2 / b**2) <= 1\n",
    "def is_outside_ellipse(x, y, a, b):\n",
    "    return (x**2 / a**2) + (y**2 / b**2) > 1\n",
    "def set_custom_view(vis):\n",
    "    ctr = vis.get_view_control()\n",
    "    \n",
    "    # Create an extrinsic matrix for camera placement\n",
    "    extrinsic = np.eye(4)\n",
    "    extrinsic[0:3, 3] = [-10, 0, 0]  # Set camera position (x, y, z)\n",
    "    \n",
    "    # Create a rotation matrix for 30-degree downward view\n",
    "    rotation = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(np.radians(-160)), -np.sin(np.radians(-160))],\n",
    "        [0, np.sin(np.radians(-160)), np.cos(np.radians(-160))]\n",
    "    ])\n",
    "    \n",
    "    # Apply rotation to the extrinsic matrix\n",
    "    extrinsic[0:3, 0:3] = rotation\n",
    "    \n",
    "    # Set the extrinsic matrix to the camera parameters\n",
    "    cam_params = ctr.convert_to_pinhole_camera_parameters()\n",
    "    cam_params.extrinsic = extrinsic\n",
    "    ctr.convert_from_pinhole_camera_parameters(cam_params)\n",
    "    opt = vis.get_render_option()\n",
    "    opt.background_color = np.asarray([0.5, 0.5, 0.5])\n",
    "\n",
    "def load_velodyne_points(filename):\n",
    "    points = np.fromfile(filename, dtype=np.float32).reshape(-1, 4)\n",
    "    return points\n",
    "def read_calib_file(calib_file):\n",
    "    with open(calib_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        calibration = {}\n",
    "\n",
    "        for line in lines:\n",
    "            # print(line)\n",
    "            try:\n",
    "                key, value = line.split(':')\n",
    "            except ValueError:\n",
    "                continue\n",
    "            calibration[key] = np.array([float(x) for x in value.strip().split()])\n",
    "        return calibration\n",
    "def compute_colors_from_distance(points,max_distance):\n",
    "    #If no disatance given return all black\n",
    "    if max_distance==None:\n",
    "        return np.zeros((points.shape[0],3))\n",
    "    distances = np.linalg.norm(points[:, :3], axis=1)\n",
    "    normalized_distances = distances / max_distance\n",
    "    return plt.cm.jet(normalized_distances)[:,:3]\n",
    "def create_point_cloud(points,distance=None):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points[:, :3])\n",
    "    colors = compute_colors_from_distance(points,distance)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    return pcd\n",
    "def plot_bounding_box_from_corners(corners,offset =0, calib=None, color=[0, 1, 0]):\n",
    "    # Define the lines connecting the corners based on their indices\n",
    "\n",
    "    lines = [[0, 1], [1, 2], [2, 3], [3, 0],\n",
    "             [4, 5], [5, 6], [6, 7], [7, 4],\n",
    "             [0, 4], [1, 5], [2, 6], [3, 7]]\n",
    "\n",
    "    # Create a LineSet object\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(corners)\n",
    "    line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "    line_set.colors = o3d.utility.Vector3dVector([color for i in range(len(lines))])\n",
    "\n",
    "    return line_set\n",
    "def filter_objects_outside_ellipse(objects, a, b,offset=5,axis=0):\n",
    "    \"\"\"\n",
    "    Filters ground truth objects to only include those outside a specified ellipse.\n",
    "\n",
    "    Parameters:\n",
    "        objects (list): The input objects, each as a dictionary.\n",
    "        a (float): Semi-major axis length of the ellipse.\n",
    "        b (float): Semi-minor axis length of the ellipse.\n",
    "\n",
    "    Returns:\n",
    "        list: The filtered objects.\n",
    "    \"\"\"\n",
    "    filtered_objects = []\n",
    "    \n",
    "    for obj in objects:\n",
    "        # print(obj)\n",
    "        if 'corners' not in obj.keys():\n",
    "            x, y = obj['x'], obj['y']\n",
    "            \n",
    "            # Generate corner points for the box\n",
    "            dx, dy = obj['dx'], obj['dy']\n",
    "\n",
    "        else:\n",
    "            x,y,_ = obj['real_center']\n",
    "        dx, dy = obj['dx'], obj['dy']\n",
    "    \n",
    "        # Check if any corner point is inside the ellipse\n",
    "        corners = np.array([\n",
    "                [x - dx/2, y - dy/2],\n",
    "                [x - dx/2, y + dy/2],\n",
    "                [x + dx/2, y - dy/2],\n",
    "                [x + dx/2, y + dy/2]\n",
    "            ])\n",
    "        adjusted_corners_x = corners[:, axis] + offset\n",
    "        inside_ellipse = is_inside_ellipse(adjusted_corners_x, corners[:, 1], a, b)\n",
    "        if np.any(inside_ellipse):\n",
    "            filtered_objects.append(obj)\n",
    "    # print(filtered_objects)\n",
    "    return filtered_objects\n",
    "\n",
    "def create_oriented_bounding_box_gt(box_params, color=(0, 1, 0),calib=None, offset=0,axis=0):\n",
    "    center = np.array([box_params['x'], box_params['y'], box_params['z']/2])\n",
    "    extent = np.array([box_params['dx'], box_params['dy'], box_params['dz']])\n",
    "    R = box_params['yaw']\n",
    "\n",
    "    # Tr_velo_to_cam = calib['Tr_velo_to_cam'].reshape(3, 4)\n",
    "    if(\"corners\" in box_params.keys()):\n",
    "    # # Convert center to homogeneous coordinates and transform to LiDAR coordinates\n",
    "    # point_homogeneous = np.hstack((center, np.ones(1)))\n",
    "    # center_tr = transform_camera_to_lidar(point_homogeneous, Tr_velo_to_cam)\n",
    "        center = box_params['real_center']\n",
    "        coners = box_params['corners']\n",
    "        # coners[:,2] /=2\n",
    "        coners[:,axis] += offset\n",
    "        return plot_bounding_box_from_corners(coners,offset=offset, calib=calib, color=color)\n",
    "        # R_empty = np.eye(3)\n",
    "        # obb = o3d.geometry.OrientedBoundingBox(center=real_center, R=R, extent=extent)\n",
    "        # cntr = o3d.geometry.OrientedBoundingBox(center=real_center, R=R, extent=[0.1,0.1,0.1])\n",
    "        # cntr.color = (1,1,1)\n",
    "        # obb.color = color\n",
    "        # return obb, cntr\n",
    "    # Rotate around the Y-axis for KITTI's rotation_y\n",
    "    # print(\"Plotting from center\")\n",
    "    # Create OrientedBoundingBox\n",
    "    obb = o3d.geometry.OrientedBoundingBox(center=center, R=R, extent=extent)\n",
    "    cntr = o3d.geometry.OrientedBoundingBox(center=center, R=R, extent=[0.1,0.1,0.1])\n",
    "    cntr.color = (1,1,1)\n",
    "    obb.color = color \n",
    "    \n",
    "    return obb\n",
    "def filter_points_inside_ellipse(points, a, b,offset=5):\n",
    "    x, y, z = points[:, 0]+offset, points[:, 1], points[:, 2]\n",
    "    inside = is_inside_ellipse(x, y, a, b)\n",
    "    return points[inside]\n",
    "def filter_points_outside_ellipse(points, a, b,offset=5):\n",
    "    x, y, z = points[:, 0]+offset, points[:, 1], points[:, 2]\n",
    "    outside = is_outside_ellipse(x, y, a, b)\n",
    "    return points[outside]\n",
    "def read_kitti_label_file(bin_file_path, filter_classes=['Car', 'Pedestrian', 'Cyclist']):\n",
    "    \"\"\"\n",
    "    Reads a KITTI label .txt file and returns a list of dictionaries containing\n",
    "    object type and bounding box parameters, filtered by specified classes.\n",
    "\n",
    "    Parameters:\n",
    "        txt_file_path (str): Path to the KITTI label .txt file.\n",
    "        filter_classes (List[str]): List of classes to include.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of dictionaries containing 'type', 'x', 'y', 'z', 'dx', 'dy', 'dz', and 'yaw'.\n",
    "    \"\"\"\n",
    "    txt_file_path = bin_file_path.replace('.bin', '.txt').replace(\"velodyne\",\"label_2\")\n",
    "    calib_file_path = bin_file_path.replace('.bin', '.txt').replace(\"velodyne\",\"calib\")\n",
    "    calib = read_calib_file(calib_file_path)\n",
    "    objects = []\n",
    "    from math import cos,sin\n",
    "\n",
    "    with open(txt_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split(' ')\n",
    "            obj_type = tokens[0]\n",
    "            if obj_type not in filter_classes:\n",
    "                continue\n",
    "            # print(line)\n",
    "            dimensions_height, dimensions_width, dimensions_length = map(float, tokens[8:11])\n",
    "            location_x, location_y, location_z = map(float, tokens[11:14])\n",
    "            center_tr = np.array([location_x, location_y, location_z])\n",
    "            rotation_y = float(tokens[14])\n",
    "            l_div_2 = dimensions_length / 2\n",
    "            x_corners = [l_div_2, l_div_2, -l_div_2, -l_div_2, l_div_2, l_div_2, -l_div_2, -l_div_2]\n",
    "            w_div_2 = dimensions_width / 2\n",
    "            y_corners = [0, 0, 0, 0, -dimensions_height, -dimensions_height, -dimensions_height, -dimensions_height]\n",
    "            z_corners = [w_div_2, -w_div_2, -w_div_2, w_div_2, w_div_2, -w_div_2, -w_div_2, w_div_2]\n",
    "            corner_matrix = np.array([x_corners, y_corners, z_corners])\n",
    "            R = np.array([[cos(rotation_y),0,sin(rotation_y)],[0,1,0],[-sin(rotation_y),0,cos(rotation_y)]])\n",
    "            rotated_corners = np.matmul(R,corner_matrix)\n",
    "            translated_corners = rotated_corners + center_tr.reshape(3,1)\n",
    "            \n",
    "            Tr_velo_to_cam = calib_data['Tr_velo_to_cam'].reshape(3, 4)\n",
    "            Tr_velo_to_cam_extended = np.eye(4)  # Create a 4x4 identity matrix\n",
    "            Tr_velo_to_cam_extended[:3, :] = Tr_velo_to_cam  # Replace the top-left 3x4 block\n",
    "            \n",
    "            # rotation_component = np.eye(4)\n",
    "            # rotation_component[:3,:3] = Tr_velo_to_cam_extended[:3,:3]\n",
    "            \n",
    "            # translation_component = np.eye(4)\n",
    "            # translation_component[:3,3] = Tr_velo_to_cam_extended[:3,3]\n",
    "            \n",
    "            # inverse_rotation = np.linalg.inv(rotation_component)\n",
    "            # inverse_translation =  -translation_component\n",
    "            \n",
    "            # new_transform = np.eye(4)\n",
    "            # new_transform[:3,:3] = inverse_rotation[:3,:3]\n",
    "            # new_transform[:3,3] = inverse_translation[:3,3]\n",
    "\n",
    "            # print(new_transform.shape,translated_corners.shape)\n",
    "            T_inv = np.linalg.inv(Tr_velo_to_cam_extended)\n",
    "            Homogeneous_corners = np.ones((4,8))\n",
    "            Homogeneous_corners[:3,:] = translated_corners\n",
    "            translated_corners = np.matmul(T_inv,Homogeneous_corners)[:3,:]\n",
    "            real_center = np.mean(translated_corners,axis=1)\n",
    "            # print(translated_corners.shape)\n",
    "            # real_center = np.mean(translated_corners,axis=1)\n",
    "            # print(real_center)\n",
    "            objects.append({\n",
    "                'type': obj_type,\n",
    "                'x': real_center[0],\n",
    "                'y': real_center[1],\n",
    "                'z': real_center[2],\n",
    "                'dx': dimensions_length,\n",
    "                'dy': dimensions_height,\n",
    "                'dz': dimensions_width,\n",
    "                'yaw': o3d.geometry.get_rotation_matrix_from_xyz((0,rotation_y,0)),\n",
    "                'corners': translated_corners.T,\n",
    "                'real_center': real_center\n",
    "            })\n",
    "    return objects\n",
    "def create_oriented_bounding_box(box_params,offset=100,axis=0,calib=None,color=(1, 0, 0)):\n",
    "    offset_array = np.zeros(3)\n",
    "    offset_array[axis] = offset\n",
    "    center = np.array([box_params[0], box_params[1], box_params[2]/2])\n",
    "    extent = np.array([box_params[3], box_params[4], box_params[5]])\n",
    "    R = o3d.geometry.get_rotation_matrix_from_xyz((0,0,box_params[6]))\n",
    "\n",
    "    # Transform_lidar_to_cam = calib['Tr_velo_to_cam'].reshape(3, 4)\n",
    "    # extended_transform = np.eye(4)\n",
    "    # extended_transform[:3, :] = Transform_lidar_to_cam\n",
    "    # corners = np.array([center + np.dot(R, np.array([x, y, z])) for x in [-extent[0], extent[0]] for y in [-extent[1], extent[1]] for z in [-extent[2], extent[2]]])\n",
    "    # center_from_corners = np.mean(corners, axis=0)\n",
    "    # center  = center_from_corners\n",
    "    center += offset_array\n",
    "    R_empty = np.eye(3)\n",
    "    obb = o3d.geometry.OrientedBoundingBox(center=center, R=R, extent=extent)\n",
    "    obb.color = (1, 0, 0)  # Red color\n",
    "    return obb\n",
    "def create_oriented_bounding_box_nuscenes(detection, offset=100, axis=0, color=(1, 0, 0),calib=None):\n",
    "    offset_array = np.zeros(3)\n",
    "    offset_array[axis] = offset\n",
    "\n",
    "    # Extract parameters from the detection array\n",
    "    x, y, z, dx, dy, dz, r, _, _ = detection\n",
    "\n",
    "    # Define the center and extent of the box\n",
    "    center = np.array([x, y, z/2])\n",
    "    extent = np.array([dx, dy, dz])\n",
    "\n",
    "    # Create the rotation matrix from the heading angle\n",
    "    R = o3d.geometry.get_rotation_matrix_from_xyz((0, 0, 0 ))\n",
    "\n",
    "    # Apply the offset to the center\n",
    "    center += offset_array\n",
    "\n",
    "    # Create the Oriented Bounding Box\n",
    "    obb = o3d.geometry.OrientedBoundingBox(center=center, R=R, extent=extent)\n",
    "    obb.color = color  # Set the color\n",
    "    obb_for_center = o3d.geometry.OrientedBoundingBox(center=center, R=R, extent=[0.1,0.1,0.1])\n",
    "    obb_for_center.color = (1,1,1)\n",
    "    return obb , obb_for_center\n",
    "def rotate_points(points, R):\n",
    "    return np.dot(points, R.T)\n",
    "\n",
    "def match_detections_3d(ground_truths, predictions, iou_threshold=0.5):\n",
    "    matches = []\n",
    "    unmatched_ground_truths = []\n",
    "    unmatched_predictions = list(predictions)\n",
    "\n",
    "    for gt in ground_truths:\n",
    "        max_iou = -1\n",
    "        max_iou_idx = -1\n",
    "        for idx, pred in enumerate(unmatched_predictions):\n",
    "            current_iou = calculate_iou_3d(gt, pred)\n",
    "            if current_iou > max_iou:\n",
    "                max_iou = current_iou\n",
    "                max_iou_idx = idx\n",
    "        # print(\"For gt\",gt,\"max iou is\",max_iou)\n",
    "        if max_iou >= iou_threshold:\n",
    "            matches.append((gt, unmatched_predictions[max_iou_idx]))\n",
    "            del unmatched_predictions[max_iou_idx]\n",
    "        else:\n",
    "            unmatched_ground_truths.append(gt)\n",
    "\n",
    "    return matches, unmatched_ground_truths, unmatched_predictions\n",
    "def calculate_iou_3d(box1,box2):\n",
    "    # Create corner points for both boxes\n",
    "    if type(box1) == o3d.geometry.OrientedBoundingBox:\n",
    "        center1 = np.array(box1.center)\n",
    "        dimensions1 = np.array(box1.extent)\n",
    "        half_dims1 = dimensions1 / 2\n",
    "        corners1 = np.array([np.array([x, y, z]) for x in [-half_dims1[0], half_dims1[0]] for y in [-half_dims1[1], half_dims1[1]] for z in [-half_dims1[2], half_dims1[2]]])\n",
    "        corners1 = rotate_points(corners1, box1.R) + center1\n",
    "\n",
    "    else:\n",
    "        #get corners from lineset\n",
    "        # print(\"Getting corners from lineset\")\n",
    "        corners1 = np.array(box1.points)\n",
    "        dimensions_from_corners = np.max(corners1,axis=0) - np.min(corners1,axis=0)\n",
    "        dimensions1 = dimensions_from_corners\n",
    "        \n",
    "    if type(box2)== o3d.geometry.OrientedBoundingBox:\n",
    "        center2 = np.array(box2.center)\n",
    "        dimensions2 = np.array(box2.extent)\n",
    "        half_dims2 = dimensions2 / 2\n",
    "        corners2 = np.array([np.array([x, y, z]) for x in [-half_dims2[0], half_dims2[0]] for y in [-half_dims2[1], half_dims2[1]] for z in [-half_dims2[2], half_dims2[2]]])\n",
    "        # Rotate and translate corners\n",
    "        corners2 = rotate_points(corners2, box2.R) + center2\n",
    "    else:\n",
    "        #get corners from lineset\n",
    "        # print(\"Getting corners from lineset\")\n",
    "\n",
    "        corners2 = np.array(box2.points)\n",
    "        dimensions_from_corners = np.max(corners2,axis=0) - np.min(corners2,axis=0)\n",
    "        dimensions2 = dimensions_from_corners\n",
    "    # Calculate axis-aligned bounding boxes for intersection\n",
    "    min_bound1 = np.min(corners1, axis=0)\n",
    "    max_bound1 = np.max(corners1, axis=0)\n",
    "    min_bound2 = np.min(corners2, axis=0)\n",
    "    max_bound2 = np.max(corners2, axis=0)\n",
    "    \n",
    "    # Calculate intersection\n",
    "    min_intersect = np.maximum(min_bound1, min_bound2)\n",
    "    max_intersect = np.minimum(max_bound1, max_bound2)\n",
    "    intersection_dims = np.maximum(0, max_intersect - min_intersect)\n",
    "    intersection_volume = np.prod(intersection_dims)\n",
    "    \n",
    "    # Calculate volumes of the individual boxes\n",
    "    vol1 = np.prod(dimensions1)\n",
    "    vol2 = np.prod(dimensions2)\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection_volume / (vol1 + vol2 - intersection_volume)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def is_point_inside_obb(obb, point):\n",
    "    \"\"\"\n",
    "    Check if a point is inside an oriented bounding box (OBB).\n",
    "    \n",
    "    Parameters:\n",
    "    - obb: Open3D OrientedBoundingBox object.\n",
    "    - point: NumPy array of shape (3,) representing the point.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the point is inside the OBB, False otherwise.\n",
    "    \"\"\"\n",
    "    # Transform the point to the OBB's local coordinate system\n",
    "    point_local = np.linalg.inv(obb.R).dot((point[:3] - obb.center))\n",
    "    \n",
    "    # Check if the transformed point is inside the axis-aligned box\n",
    "    return np.all(np.abs(point_local) <= (obb.extent / 2))\n",
    "\n",
    "def remove_points_inside_obbs(point_cloud, obbs):\n",
    "    \"\"\"\n",
    "    Remove points inside oriented bounding boxes from a point cloud.\n",
    "    \n",
    "    Parameters:\n",
    "    - point_cloud: NumPy array of shape (N, 3), where N is the number of points.\n",
    "    - obbs: List of Open3D OrientedBoundingBox objects.\n",
    "    \n",
    "    Returns:\n",
    "    - filtered_point_cloud: NumPy array containing points outside the bounding boxes.\n",
    "    \"\"\"\n",
    "    mask = np.ones(point_cloud.shape[0], dtype=bool)\n",
    "    \n",
    "    for obb in obbs:\n",
    "        for i, point in enumerate(point_cloud):\n",
    "            if is_point_inside_obb(obb, point):\n",
    "                mask[i] = False\n",
    "    \n",
    "    filtered_point_cloud = point_cloud[mask]\n",
    "    \n",
    "    return filtered_point_cloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mmdetection3d\\mmdet3d\\evaluation\\functional\\kitti_utils\\eval.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def get_thresholds(scores: np.ndarray, num_gt, num_sample_pts=41):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import open3d as o3d\n",
    "import mmdet3d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from plyfile import PlyData\n",
    "from mmcv.transforms.base import BaseTransform\n",
    "from mmengine.registry import TRANSFORMS, VISUALIZERS\n",
    "from mmengine.structures import InstanceData\n",
    "from mmdet3d.utils import register_all_modules\n",
    "from mmdet3d.apis import inference_detector, init_model\n",
    "from mmdet3d.evaluation.metrics.nuscenes_metric import NuScenesMetric\n",
    "from mmdet3d.datasets import NuScenesDataset\n",
    "from mmdet3d.structures import Det3DDataSample, LiDARInstance3DBoxes\n",
    "from mmdet3d.visualization import Det3DLocalVisualizer\n",
    "from mmdet3d.structures import LiDARInstance3DBoxes\n",
    "def load_velodyne_points(filename):\n",
    "    points = np.fromfile(filename, dtype=np.float32).reshape(-1, 4)\n",
    "    return points\n",
    "def read_calib_file(calib_file):\n",
    "    with open(calib_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        calibration = {}\n",
    "\n",
    "        for line in lines:\n",
    "            # print(line)\n",
    "            try:\n",
    "                key, value = line.split(':')\n",
    "            except ValueError:\n",
    "                continue\n",
    "            calibration[key] = np.array([float(x) for x in value.strip().split()])\n",
    "        return calibration\n",
    "def read_kitti_label_file(bin_file_path, filter_classes=['Car', 'Pedestrian', 'Cyclist']):\n",
    "    \"\"\"\n",
    "    Reads a KITTI label .txt file and returns a list of dictionaries containing\n",
    "    object type and bounding box parameters, filtered by specified classes.\n",
    "\n",
    "    Parameters:\n",
    "        txt_file_path (str): Path to the KITTI label .txt file.\n",
    "        filter_classes (List[str]): List of classes to include.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of dictionaries containing 'type', 'x', 'y', 'z', 'dx', 'dy', 'dz', and 'yaw'.\n",
    "    \"\"\"\n",
    "    txt_file_path = bin_file_path.replace('.bin', '.txt').replace(\"velodyne\",\"label_2\")\n",
    "    calib_file_path = bin_file_path.replace('.bin', '.txt').replace(\"velodyne\",\"calib\")\n",
    "    calib = read_calib_file(calib_file_path)\n",
    "    objects = []\n",
    "    from math import cos,sin\n",
    "\n",
    "    with open(txt_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split(' ')\n",
    "            obj_type = tokens[0]\n",
    "            if obj_type not in filter_classes:\n",
    "                continue\n",
    "            # print(line)\n",
    "            dimensions_height, dimensions_width, dimensions_length = map(float, tokens[8:11])\n",
    "            location_x, location_y, location_z = map(float, tokens[11:14])\n",
    "            center_tr = np.array([location_x, location_y, location_z])\n",
    "            rotation_y = float(tokens[14])\n",
    "            l_div_2 = dimensions_length / 2\n",
    "            x_corners = [l_div_2, l_div_2, -l_div_2, -l_div_2, l_div_2, l_div_2, -l_div_2, -l_div_2]\n",
    "            w_div_2 = dimensions_width / 2\n",
    "            y_corners = [0, 0, 0, 0, -dimensions_height, -dimensions_height, -dimensions_height, -dimensions_height]\n",
    "            z_corners = [w_div_2, -w_div_2, -w_div_2, w_div_2, w_div_2, -w_div_2, -w_div_2, w_div_2]\n",
    "            corner_matrix = np.array([x_corners, y_corners, z_corners])\n",
    "            R = np.array([[cos(rotation_y),0,sin(rotation_y)],[0,1,0],[-sin(rotation_y),0,cos(rotation_y)]])\n",
    "            rotated_corners = np.matmul(R,corner_matrix)\n",
    "            translated_corners = rotated_corners + center_tr.reshape(3,1)\n",
    "            \n",
    "            Tr_velo_to_cam = calib['Tr_velo_to_cam'].reshape(3, 4)\n",
    "            Tr_velo_to_cam_extended = np.eye(4)  # Create a 4x4 identity matrix\n",
    "            Tr_velo_to_cam_extended[:3, :] = Tr_velo_to_cam  # Replace the top-left 3x4 block\n",
    "            \n",
    "            # rotation_component = np.eye(4)\n",
    "            # rotation_component[:3,:3] = Tr_velo_to_cam_extended[:3,:3]\n",
    "            \n",
    "            # translation_component = np.eye(4)\n",
    "            # translation_component[:3,3] = Tr_velo_to_cam_extended[:3,3]\n",
    "            \n",
    "            # inverse_rotation = np.linalg.inv(rotation_component)\n",
    "            # inverse_translation =  -translation_component\n",
    "            \n",
    "            # new_transform = np.eye(4)\n",
    "            # new_transform[:3,:3] = inverse_rotation[:3,:3]\n",
    "            # new_transform[:3,3] = inverse_translation[:3,3]\n",
    "\n",
    "            # print(new_transform.shape,translated_corners.shape)\n",
    "            T_inv = np.linalg.inv(Tr_velo_to_cam_extended)\n",
    "            Homogeneous_corners = np.ones((4,8))\n",
    "            Homogeneous_corners[:3,:] = translated_corners\n",
    "            translated_corners = np.matmul(T_inv,Homogeneous_corners)[:3,:]\n",
    "            real_center = np.mean(translated_corners,axis=1)\n",
    "            # print(translated_corners.shape)\n",
    "            # real_center = np.mean(translated_corners,axis=1)\n",
    "            # print(real_center)\n",
    "            objects.append({\n",
    "                'type': obj_type,\n",
    "                'x': real_center[0],\n",
    "                'y': real_center[1],\n",
    "                'z': real_center[2],\n",
    "                'dx': dimensions_length,\n",
    "                'dy': dimensions_height,\n",
    "                'dz': dimensions_width,\n",
    "                'yaw': o3d.geometry.get_rotation_matrix_from_xyz((0,rotation_y,0)),\n",
    "                'corners': translated_corners.T,\n",
    "                'real_center': real_center\n",
    "            })\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mmdetection3d\\mmdet3d\\models\\dense_heads\\anchor3d_head.py:92: UserWarning: dir_offset and dir_limit_offset will be depressed and be incorporated into box coder in the future\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: D:\\mmdetection3d/ckpts/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47abe4c622af4d218f325451512cac48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yatba\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Introspect3D\\development_plaground.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m filtered_boxes \u001b[39m=\u001b[39m pred_boxes_box_f[filtered_indices]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m obb_list_f \u001b[39m=\u001b[39m [create_oriented_bounding_box(box,offset\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,calib\u001b[39m=\u001b[39mcalib_data) \u001b[39mfor\u001b[39;00m box \u001b[39min\u001b[39;00m filtered_boxes]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m removed_object_points \u001b[39m=\u001b[39m remove_points_inside_obbs(filtered_points, obb_list_f)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m flag \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# removed_pcd = create_point_cloud(removed_object_points,distance=max_distance)\u001b[39;00m\n",
      "\u001b[1;32md:\\Introspect3D\\development_plaground.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=404'>405</a>\u001b[0m \u001b[39mfor\u001b[39;00m obb \u001b[39min\u001b[39;00m obbs:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=405'>406</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, point \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(point_cloud):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=406'>407</a>\u001b[0m         \u001b[39mif\u001b[39;00m is_point_inside_obb(obb, point):\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=407'>408</a>\u001b[0m             mask[i] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=409'>410</a>\u001b[0m filtered_point_cloud \u001b[39m=\u001b[39m point_cloud[mask]\n",
      "\u001b[1;32md:\\Introspect3D\\development_plaground.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=375'>376</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=376'>377</a>\u001b[0m \u001b[39mCheck if a point is inside an oriented bounding box (OBB).\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=377'>378</a>\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=383'>384</a>\u001b[0m \u001b[39m- bool: True if the point is inside the OBB, False otherwise.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=384'>385</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=385'>386</a>\u001b[0m \u001b[39m# Transform the point to the OBB's local coordinate system\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=386'>387</a>\u001b[0m point_local \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49minv(obb\u001b[39m.\u001b[39;49mR)\u001b[39m.\u001b[39;49mdot((point[:\u001b[39m3\u001b[39;49m] \u001b[39m-\u001b[39;49m obb\u001b[39m.\u001b[39;49mcenter))\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=388'>389</a>\u001b[0m \u001b[39m# Check if the transformed point is inside the axis-aligned box\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Introspect3D/development_plaground.ipynb#W5sZmlsZQ%3D%3D?line=389'>390</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mall(np\u001b[39m.\u001b[39mabs(point_local) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m (obb\u001b[39m.\u001b[39mextent \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "kitti_velodyne_path= r\"D:\\introspectionBase\\datasets\\Kitti\\raw\\training\\velodyne\"\n",
    "dataset_path = r\"D:\\Introspect3D\\custom_dataset\\pointpillars_kitti_class3\"\n",
    "flag = 0\n",
    "\n",
    "def myhook(module, input, output):\n",
    "    global im_name,dataset_path,flag\n",
    "    # print(output[0].shape,output[1].shape,output[2].shape)\n",
    "    numpy_output = output[2].squeeze().cpu().detach().numpy()\n",
    "    # print(numpy_output.shape)\n",
    "    # print(f\"Saving to {os.path.join(dataset_path,im_name.replace('.png','.npy'))}\")\n",
    "    if(flag):\n",
    "        np.save(os.path.join(dataset_path,'features',im_name.replace(\".png\",\".npy\")),numpy_output)\n",
    "    else:\n",
    "        np.save(os.path.join(dataset_path,'features_removed',im_name.replace(\".png\",\".npy\")),numpy_output)\n",
    "def read_calib_file(calib_file):\n",
    "    with open(calib_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        calibration = {}\n",
    "\n",
    "        for line in lines:\n",
    "            # print(line)\n",
    "            try:\n",
    "                key, value = line.split(':')\n",
    "            except ValueError:\n",
    "                continue\n",
    "            calibration[key] = np.array([float(x) for x in value.strip().split()])\n",
    "        return calibration\n",
    "if __name__ == \"__main__\":\n",
    "    # build the model from a config file and a checkpoint file\n",
    "    config_file = r'D:\\mmdetection3d\\configs\\pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py'\n",
    "    # download the checkpoint from model zoo and put it in `checkpoints/`\n",
    "    checkpoint = r'D:\\mmdetection3d/ckpts/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth'\n",
    "    model = init_model(config_file, checkpoint, device='cuda:0')\n",
    "\n",
    "    hook = model.backbone.register_forward_hook(myhook)\n",
    "    score_threshold = 0.5\n",
    "    with tqdm(total=7480) as pbar:\n",
    "        for i in range(7480):\n",
    "            test_num  = i\n",
    "            filename = os.path.join(kitti_velodyne_path, f'{test_num:06}.bin')\n",
    "            calib_data = read_calib_file(filename.replace('.bin', '.txt').replace(\"velodyne\",\"calib\"))\n",
    "            points = load_velodyne_points(filename)\n",
    "\n",
    "            im_name = f'{test_num:06}.png'           \n",
    "            # Transformation matrix is in (3x4) shape, so we extend it to (4x4) by adding a row of [0,0,0,1] for homogeneous coordinates\n",
    "            Tr_velo_to_cam = calib_data['Tr_velo_to_cam'].reshape(3, 4)\n",
    "            Tr_velo_to_cam_extended = np.eye(4)  # Create a 4x4 identity matrix\n",
    "            Tr_velo_to_cam_extended[:3, :] = Tr_velo_to_cam  # Replace the top-left 3x4 block\n",
    "            \n",
    "            max_distance = np.max(np.linalg.norm(points[:, :3], axis=1))\n",
    "            a, b = 25, 15  # Semi-major and semi-minor axes\n",
    "            filtered_points = filter_points_inside_ellipse(points, a, b,offset=-10) #filter_points_inside_pyramid(points, min_x, max_x, min_y,max_y ) #\n",
    "            filtered_pcd = create_point_cloud(filtered_points,distance=max_distance)\n",
    "            outside_points = filter_points_outside_ellipse(filtered_points, a, b,offset=-10)\n",
    "            outside_pcd = create_point_cloud(outside_points)\n",
    "            flag = 1\n",
    "            res_f,data_f = inference_detector(model, filtered_points)\n",
    "            pred_boxes_box_f = res_f.pred_instances_3d.bboxes_3d.tensor.cpu().numpy()\n",
    "            pred_boxes_score_f = res_f.pred_instances_3d.scores_3d.cpu().numpy()\n",
    "            filtered_indices = np.where(pred_boxes_score_f >= score_threshold)[0]\n",
    "            filtered_boxes = pred_boxes_box_f[filtered_indices]\n",
    "            obb_list_f = [create_oriented_bounding_box(box,offset=0,calib=calib_data) for box in filtered_boxes]\n",
    "            removed_object_points = remove_points_inside_obbs(filtered_points, obb_list_f)\n",
    "            flag = 0\n",
    "            # removed_pcd = create_point_cloud(removed_object_points,distance=max_distance)\n",
    "            one_more_chance_res, one_more_chance_data = inference_detector(model, removed_object_points)\n",
    "            # one_more_chance_pred_boxes_box = one_more_chance_res.pred_instances_3d.bboxes_3d.tensor.cpu().numpy()\n",
    "            # one_more_chance_pred_boxes_score = one_more_chance_res.pred_instances_3d.scores_3d.cpu().numpy()\n",
    "            # filtered_indices = np.where(one_more_chance_pred_boxes_score >= score_threshold)[0]\n",
    "            # one_more_chance_filtered_boxes = one_more_chance_pred_boxes_box[filtered_indices]\n",
    "            # one_more_chance_obb_list_f = [create_oriented_bounding_box(box,offset=0,calib=calib_data,color=(1,0.647,0)) for box in one_more_chance_filtered_boxes]\n",
    "\n",
    "            # test a single sample\n",
    "            # print(points.shape)\n",
    "            # points = np.expand_dims(points,axis=0)\n",
    "            # res = inference_detector(model,points)  \n",
    "            pbar.update(1)\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: D:\\mmdetection3d/ckpts/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mmdetection3d\\mmdet3d\\models\\dense_heads\\anchor3d_head.py:92: UserWarning: dir_offset and dir_limit_offset will be depressed and be incorporated into box coder in the future\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "    config_file = r'D:\\mmdetection3d\\configs\\pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py'\n",
    "    # download the checkpoint from model zoo and put it in `checkpoints/`\n",
    "    checkpoint = r'D:\\mmdetection3d/ckpts/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth'\n",
    "    model = init_model(config_file, checkpoint, device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VoxelNet(\n",
       "  (data_preprocessor): Det3DDataPreprocessor(\n",
       "    (voxel_layer): VoxelizationByGridShape(voxel_size=[0.16, 0.16, 4], grid_shape=[432, 496, 1], point_cloud_range=[0, -39.68, -3, 69.12, 39.68, 1], max_num_points=32, max_voxels=(16000, 40000), deterministic=True)\n",
       "  )\n",
       "  (backbone): SECOND(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (10): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (11): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (10): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (13): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (14): ReLU(inplace=True)\n",
       "        (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (16): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (17): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (10): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (13): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (14): ReLU(inplace=True)\n",
       "        (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (16): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (17): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}\n",
       "  (neck): SECONDFPN(\n",
       "    (deblocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  init_cfg=[{'type': 'Kaiming', 'layer': 'ConvTranspose2d'}, {'type': 'Constant', 'layer': 'NaiveSyncBatchNorm2d', 'val': 1.0}]\n",
       "  (bbox_head): Anchor3DHead(\n",
       "    (loss_cls): FocalLoss()\n",
       "    (loss_bbox): SmoothL1Loss()\n",
       "    (loss_dir): CrossEntropyLoss(avg_non_ignore=False)\n",
       "    (conv_cls): Conv2d(384, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv_reg): Conv2d(384, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv_dir_cls): Conv2d(384, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  init_cfg={'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01, 'override': {'type': 'Normal', 'name': 'conv_cls', 'std': 0.01, 'bias_prob': 0.01}}\n",
       "  (voxel_encoder): PillarFeatureNet(\n",
       "    (pfn_layers): ModuleList(\n",
       "      (0): PFNLayer(\n",
       "        (norm): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (linear): Linear(in_features=10, out_features=64, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle_encoder): PointPillarsScatter()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "arr = np.array(np.random.rand(8,3))\n",
    "offset = np.array([0,10,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "<>:4: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "/tmp/ipykernel_3561194/2971988303.py:4: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "  eval('f'(5))\n",
      "/tmp/ipykernel_3561194/2971988303.py:4: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "  eval('f'(5))\n",
      "/tmp/ipykernel_3561194/2971988303.py:4: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "  eval('f'(5))\n",
      "/tmp/ipykernel_3561194/2971988303.py:4: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "  eval('f'(5))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/ssd1/Introspect3D/development_plaground.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/ssd1/Introspect3D/development_plaground.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(x):\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/ssd1/Introspect3D/development_plaground.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(x)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/ssd1/Introspect3D/development_plaground.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39meval\u001b[39m(\u001b[39m'\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m(\u001b[39m5\u001b[39;49m))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    print(x)\n",
    "\n",
    "eval('f'(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
