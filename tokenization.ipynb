{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class ActivationMapTokenizer(nn.Module):\n",
    "    def __init__(self, C, H, W, embed_dim):\n",
    "        super(ActivationMapTokenizer, self).__init__()\n",
    "        self.C = C\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Linear projection of flattened tokens to embed_dim\n",
    "        self.projection = nn.Linear(H * W, embed_dim)\n",
    "        self.norm = nn.LayerNorm(embed_dim)  # Adding LayerNorm for stabilization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input activation map with shape (batch_size, C, H, W)\n",
    "        batch_size = x.size(0)\n",
    "        # Flatten each channel to create tokens of shape (batch_size, C, H*W)\n",
    "        x = x.view(batch_size, self.C, -1)\n",
    "        # Project flattened tokens to the desired embedding dimension\n",
    "        x = self.projection(x)  # Now shape (batch_size, C, embed_dim)\n",
    "        x = self.norm(x)  # Apply normalization\n",
    "        return x\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, C, embed_dim):\n",
    "        super(SinusoidalPositionalEncoding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Create a buffer for the positional encodings\n",
    "        position = torch.arange(0, C).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        # Compute the sinusoidal encodings\n",
    "        pe = torch.zeros(C, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Register as a buffer so it is not considered a parameter but is still moved to the correct device\n",
    "        self.register_buffer('positional_encoding', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input tensor with shape (batch_size, C, embed_dim)\n",
    "        x = x + self.positional_encoding\n",
    "        return x\n",
    "\n",
    "class CustomTransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, C, H, W):\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        self.tokenizer = ActivationMapTokenizer(C, H, W, embed_dim)\n",
    "        \n",
    "        # Use Sinusoidal Positional Encoding\n",
    "        self.positional_encoding = SinusoidalPositionalEncoding(C, embed_dim)\n",
    "        \n",
    "        self.transformer_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.transformer_layers, num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Error detection head\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),  # Adding a dense layer with ReLU activation\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),  # Adding dropout for regularization\n",
    "            nn.Linear(128, 2)  # Output layer for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Error detection output\n",
    "        x = self.classification_head(x.mean(dim=1))  # Aggregate over channels\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model = CustomTransformerEncoder(embed_dim=512, num_heads=8, num_layers=4, C=256, H=180, W=180)\n",
    "activation_maps = torch.randn(8, 256, 180, 180)  # Example batch\n",
    "output = model(activation_maps)\n",
    "print(output.shape)  # Output shape should be (batch_size, 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
